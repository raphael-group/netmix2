{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T15:28:34.576662Z",
     "start_time": "2021-03-09T15:28:31.731599Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import hmean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import *\n",
    "import scipy as sp\n",
    "import csv\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import maximum_flow\n",
    "from scipy.optimize import minimize, LinearConstraint, Bounds, dual_annealing, brute, basinhopping, shgo\n",
    "\n",
    "# import graph_tool.all as gt\n",
    "\n",
    "from time import time\n",
    "import os\n",
    "\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T15:28:45.238146Z",
     "start_time": "2021-03-09T15:28:45.194288Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def shift_resps(resps, clique_size):\n",
    "    return resps - np.flip(np.sort(resps))[clique_size]\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# probably should have named this normalized\n",
    "def cutsize(S, A):\n",
    "    n = A.shape[0]\n",
    "    notS = [i for i in range(n) if i not in S]\n",
    "    return np.sum(A[np.ix_(S, notS)]) / len(S)\n",
    "\n",
    "# lmao this naming fuck me\n",
    "def cut(S,A):\n",
    "    n = A.shape[0]\n",
    "    notS = [i for i in range(n) if i not in S]\n",
    "    return np.sum(A[np.ix_(S, notS)])\n",
    "\n",
    "def edge_density(S,A):\n",
    "    return 0.5*np.sum(A[np.ix_(S, S )]) / (0.5*(len(S))*(len(S)-1))\n",
    "\n",
    "def edge_density2(S,A):\n",
    "    return 0.5*np.sum(A[np.ix_(S, S )]) / (len(S))\n",
    "\n",
    "def edge_density_min(S,A):\n",
    "    AS = np.minimum( A[np.ix_(S, S )], A[np.ix_(S, S )].T )\n",
    "    m = AS.shape[0]\n",
    "    return np.sum(AS[np.triu_indices(m,1)]) / m\n",
    "    \n",
    "\n",
    "##############################################################################\n",
    "\n",
    "def precision_recall(pred_set,act_set):\n",
    "    true_positives = np.intersect1d(pred_set, act_set)\n",
    "\n",
    "    prec = len(true_positives) / len(pred_set)\n",
    "    recall = len(true_positives) / len(act_set)\n",
    "\n",
    "    return prec, recall\n",
    "\n",
    "def fmeasure(pred_set,act_set):\n",
    "    if len(pred_set) == 0 or len(act_set) == 0:\n",
    "        return 0\n",
    "    true_positives = np.intersect1d(pred_set, act_set)\n",
    "\n",
    "    prec = len(true_positives) / len(pred_set)\n",
    "    recall = len(true_positives) / len(act_set)\n",
    "\n",
    "#     print('precision: {}'.format(prec))\n",
    "#     print('recall: {}'.format(recall))\n",
    "\n",
    "    if prec == 0 or recall == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return hmean([prec, recall])  \n",
    "\n",
    "def tpr_fpr(pred_set,act_set,n):\n",
    "    true_positives = np.intersect1d(pred_set, act_set)\n",
    "\n",
    "    tpr = len(true_positives) / len(act_set)\n",
    "    fpr = (len(pred_set) - len(true_positives)) / (n-len(act_set))\n",
    "\n",
    "    return tpr,fpr\n",
    "    \n",
    "##############################################################################\n",
    "\n",
    "def pdf(x, mu=0.0, sigma=1.0):\n",
    "    return sp.stats.norm.pdf(x, mu, sigma)\n",
    "\n",
    "def log_likelihood(x, mu, pi):\n",
    "    return np.nansum(np.log(pi*pdf(x, mu)+(1-pi)*pdf(x)))\n",
    "\n",
    "def single_em(x, mu=0.0, pi=0.05, tol=1e-3, max_num_iter=10**3):\n",
    "    x = np.asarray(x)\n",
    "    a = np.zeros(np.shape(x))\n",
    "    b = np.zeros(np.shape(x))\n",
    "    gamma = np.zeros(np.shape(x))\n",
    "    n = np.size(x)\n",
    "\n",
    "    previous_log_likelihood = log_likelihood(x, mu, pi)\n",
    "\n",
    "    for _ in range(max_num_iter):\n",
    "        # Perform E step.\n",
    "        a[:] = pi*pdf(x, mu)\n",
    "        b[:] = (1-pi)*pdf(x)\n",
    "        gamma[:] = a/(a+b)\n",
    "\n",
    "        # Perform M step.\n",
    "        sum_gamma = np.sum(gamma)\n",
    "        mu = np.sum(gamma*x)/sum_gamma\n",
    "        pi = sum_gamma/n\n",
    "\n",
    "        # Check for convergence.\n",
    "        current_log_likelihood = log_likelihood(x, mu, pi)\n",
    "        if current_log_likelihood<(1+tol)*previous_log_likelihood:\n",
    "            break\n",
    "        else:\n",
    "            previous_log_likelihood = current_log_likelihood\n",
    "\n",
    "    return mu, pi\n",
    "\n",
    "# DONT put alpha=0 into alpha_list\n",
    "def em(x, alpha_list, tol=1e-3, max_num_iter=10**3):\n",
    "    sorted_x = np.sort(np.asarray(x).flatten())[::-1]\n",
    "    n = x.shape[0]\n",
    "    \n",
    "    best_mu=0\n",
    "    best_alpha=0\n",
    "    best_ll = -np.Inf\n",
    "\n",
    "    for alpha0 in alpha_list:\n",
    "        mu0 = np.mean(sorted_x[:int(n*alpha0)])\n",
    "#         print('alpha0: {}, mu0: {}'.format(alpha0, mu0))\n",
    "        mu, alpha = single_em(x, mu=mu0, pi=alpha0, tol=tol, max_num_iter=max_num_iter)\n",
    "        ll = log_likelihood(x, mu, alpha)\n",
    "#         print('mu: {}, alpha: {}'.format(mu, alpha))\n",
    "        if ll > best_ll:\n",
    "            best_mu=mu\n",
    "            best_alpha=alpha\n",
    "            best_ll=ll\n",
    "#         print('')\n",
    "    return best_mu, best_alpha\n",
    "\n",
    "def compute_responsibilities(A, mu, alpha):\n",
    "    top = alpha*pdf(A-mu)\n",
    "    bottom = top + (1-alpha)*pdf(A)\n",
    "    return top/bottom\n",
    "\n",
    "def compute_logliks(A, mu, alpha):\n",
    "    top = alpha*pdf(A-mu)\n",
    "    bottom = (1-alpha)*pdf(A)\n",
    "    return np.log(top) - np.log(bottom)\n",
    "\n",
    "######################################################\n",
    "\n",
    "def write_list_to_file(filename, l):\n",
    "    with open(filename, 'wt') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        for i in range(len(l)):\n",
    "            tsv_writer.writerow([ str(l[i]) ])\n",
    "            \n",
    "def write_list_labels_to_file(filename, labels, l):\n",
    "    with open(filename, 'wt') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        for i in range(len(l)):\n",
    "            tsv_writer.writerow([ str(labels[i]), str(l[i]) ])\n",
    "\n",
    "def write_num_to_file(filename, num):\n",
    "    with open(filename, 'wt') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow([ str(num) ])\n",
    "\n",
    "# put all list items in file, assuming labels dont matter\n",
    "def read_labels_list_from_file(anomaly_file):\n",
    "    L = []\n",
    "    with open(anomaly_file) as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            L.append(float(row[1]))\n",
    "    return L\n",
    "        \n",
    "def read_anomaly_from_file(anomaly_file):\n",
    "    L = []\n",
    "    with open(anomaly_file) as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            L.append(int(row[0]))\n",
    "    return L\n",
    "\n",
    "def read_num_from_file(filename):\n",
    "    with open(filename) as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            return float(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetMix Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T20:36:02.472315Z",
     "start_time": "2021-03-10T20:36:02.451639Z"
    }
   },
   "outputs": [],
   "source": [
    "def netmix_ppr_clique_gurobi(resps, max_clique_list, mc_membership_dict, num_cliques_ub, total_clique_size, disjoint=False, time_limit=None, output=False):\n",
    "    n = resps.shape[0]\n",
    "    m = len(max_clique_list)\n",
    "    \n",
    "    model = gp.Model(\"anomaly\")\n",
    "    if not output:\n",
    "        model.setParam('OutputFlag',0)\n",
    "    \n",
    "    if time_limit is not None:\n",
    "        model.setParam('TimeLimit', time_limit)\n",
    "\n",
    "    # CREATE VARIABLES\n",
    "    x_inds = [i for i in range(n)] # vertex variables\n",
    "    x=model.addVars(x_inds, vtype=GRB.BINARY, name=\"x\")\n",
    "#     model.update()\n",
    "    \n",
    "    y_inds = [i for i in range(m)]\n",
    "    y=model.addVars(y_inds, vtype=GRB.BINARY, name=\"y\")\n",
    "    \n",
    "    # OBJECTIVE\n",
    "    w = {i:resps[i] for i in x_inds}\n",
    "    obj_exp = x.prod(w)\n",
    "\n",
    "    model.setObjective(obj_exp, GRB.MAXIMIZE)\n",
    "    \n",
    "    # CONSTRAINT ON MAX NUMBER OF NODES\n",
    "    model.addConstr( quicksum([x[i] for i in x_inds]) <= total_clique_size )\n",
    "    \n",
    "    # CONSTRAINT ON NUMBER OF CLIQUES\n",
    "    model.addConstr( quicksum([y[j] for j in y_inds]) <= num_cliques_ub )\n",
    "    model.addConstr( 1 <= quicksum([y[j] for j in y_inds]) )\n",
    "    \n",
    "    # CONSTRAINTS ON CLIQUES\n",
    "    # |C_j| * y_j = sum_{i in S_j} x_i for all cliques j\n",
    "    for j in y_inds:\n",
    "        Cj = max_clique_list[j]\n",
    "        model.addConstr( len(Cj) * y[j] <= quicksum([ x[i] for i in Cj ]) )\n",
    "\n",
    "    # sum_{C_j containing i} y_j >= x_i\n",
    "    # so x[i] = 1 only if some clique j contains it and is selected\n",
    "    for i in x_inds:\n",
    "        model.addConstr( x[i] <= quicksum([y[j] for j in mc_membership_dict[i]]) )\n",
    "        \n",
    "    if disjoint:\n",
    "        model.addConstr( quicksum([x[i] for i in x_inds]) == quicksum([len(max_clique_list[j])*y[j] for j in y_inds]) )\n",
    "    \n",
    "    # OPTIMIZE MODEL\n",
    "    model.optimize()\n",
    "    \n",
    "    try:\n",
    "        return [i for i in x_inds if x[i].X > 0.99], [j for j in y_inds if y[j].X > 0.99]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T15:47:40.453379Z",
     "start_time": "2021-03-09T15:47:17.440146Z"
    }
   },
   "outputs": [],
   "source": [
    "node_list_file = '/n/fs/ragr-research/projects/netsd/data/network/hint+hi-iii_processed-node_list.tsv'\n",
    "edge_list_file = '/n/fs/ragr-research/projects/netsd/data/network/hint+hi-iii_processed.tsv'\n",
    "\n",
    "node_list = []\n",
    "# tyler left some singletons in the graph so going to get rid of them.\n",
    "# after getting rid of these 84 nodes the graph has one connected component\n",
    "singleton_nodes=['ABTB2', 'ACSBG2', 'ACTG2', 'ADGRF4', 'ANKRD42', 'ATP5F1C', 'BEND4', 'BMP5', 'BTBD7', 'C10orf120', 'C2orf76', 'C9orf64', 'CCDC39', 'CDH9', 'CHRNB4', 'CNTN4', 'DCHS2', 'DDO', 'ECSCR', 'HSD11B1L', 'MNX1', 'CPNE9', 'KBTBD8', 'KCTD11', 'KLHL28', 'KNDC1', 'NYAP1', 'OR2T35', 'PTPRT', 'TMTC2', 'UST', 'DNAJC22', 'DSC3', 'EME2', 'FAM173B', 'FBXO39', 'FDX2', 'GABRB2', 'GJA3', 'H1FNT', 'HDAC10', 'HIST1H2BD', 'IAH1', 'IL36G', 'NUDT16', 'SH3D21', 'SLC32A1', 'KRTAP9-1', 'KRTAP9-7', 'KRTAP9-9', 'LRRC27', 'LRRC52', 'LY6G6D', 'PRDM8', 'RGPD2', 'SLC17A4', 'SLC22A17', 'SMTNL2', 'SYCP1', 'TDRD5', 'USP17L1', 'USP17L5', 'MMP25', 'NAA60', 'NALCN', 'NPIPB5', 'OR5K2', 'PQLC2L', 'PXT1', 'RBPJL', 'SCIMP', 'SFTPB', 'SLC24A5', 'SLC9A8', 'STC1', 'TCTEX1D1', 'TEX47', 'TMPRSS11E', 'WFDC9', 'ZNF248', 'ZNF404', 'OTUD3', 'TRPV4', 'TTYH2']\n",
    "\n",
    "# read node_list_file\n",
    "node_list = []\n",
    "with open(node_list_file) as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        if row[0][0] != '#' and row[0] not in singleton_nodes:\n",
    "            node_list.append(row[0])\n",
    "\n",
    "n = len(node_list)\n",
    "# read edge_list_file\n",
    "A = np.zeros((n,n))\n",
    "with open(edge_list_file) as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        if row[0][0] != '#':\n",
    "            if row[0] not in singleton_nodes and row[1] not in singleton_nodes:\n",
    "                i = node_list.index(row[0])\n",
    "                j = node_list.index(row[1])\n",
    "                A[i,j] = 1\n",
    "                A[j,i] = 1\n",
    "degs=np.sum(A,0)\n",
    "num_edges=int(np.sum(A)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T15:31:45.847870Z",
     "start_time": "2021-03-09T15:30:17.359928Z"
    }
   },
   "outputs": [],
   "source": [
    "# get walk matrix, PPR matrix\n",
    "r=0.4\n",
    "D = np.diag(np.sum(A,axis=0))\n",
    "\n",
    "P = A.dot(np.linalg.inv(D))\n",
    "PPR_mat = r * np.linalg.inv(np.eye(n) - (1-r)*P)\n",
    "\n",
    "# laplacian\n",
    "L = D - A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T15:43:12.159960Z",
     "start_time": "2021-03-09T15:42:51.582310Z"
    }
   },
   "outputs": [],
   "source": [
    "# get sorted list of min(PPR_mat[u,v], PPR_mat[v,u])\n",
    "PPRmin = np.minimum(PPR_mat, PPR_mat.T)\n",
    "PPRmin_nodiag = PPRmin - np.diag(np.diag(PPRmin))\n",
    "\n",
    "# edge weights min( PPR_mat[u,v], PPR_mat[v,u]) sorted in decreasing order\n",
    "PPRmin_nodiag_sorted=np.sort(PPRmin_nodiag[np.triu_indices(PPRmin_nodiag.shape[0],1)])[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run NetMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T16:24:35.477368Z",
     "start_time": "2021-03-09T16:24:35.473182Z"
    }
   },
   "outputs": [],
   "source": [
    "## PARAMETERS\n",
    "\n",
    "# delta, or the minimum PPR threshold\n",
    "# we create NEW graph G_delta which has edge (u,v) if PPRmat[u,v] > delta and PPRmat[v,u] > delta\n",
    "# we choose delta so that number of edges in Gdelta ~= number of edges in G\n",
    "delta=PPRmin_nodiag_sorted[num_edges]\n",
    "\n",
    "# k, or the number of altered subnetworks (ie cliques in G_delta) we aim to find\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T16:10:24.637510Z",
     "start_time": "2021-03-09T16:10:15.236120Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. create G_delta, new graph with edge between u and v <---> min(PPRmat[u,v], PPRmat[v,u]) > delta\n",
    "PPRmin_nodiag_delta = PPRmin_nodiag * (PPRmin_nodiag > delta)\n",
    "\n",
    "Adelta=np.zeros((n,n))\n",
    "Adelta[np.nonzero(PPRmin_nodiag_delta)]=1 # adjacency matrix for Gdelta\n",
    "Gdelta=nx.Graph(Adelta)\n",
    "\n",
    "# get max cliques (with size above 5) from Gdelta\n",
    "max_cliques_delta=list(nx.find_cliques(Gdelta))\n",
    "max_cliques_delta_sorted=sorted(max_cliques_delta,key=len)[::-1]\n",
    "max_cliques_delta_sorted_sizeabove5 = [S for S in max_cliques_delta_sorted if len(S) >= 5]\n",
    "\n",
    "# create dict { v: [list of cliques containing v] }\n",
    "mc_membership_dict={i:[] for i in range(n)}\n",
    "for t,mc in enumerate(max_cliques_delta_sorted_sizeabove5):\n",
    "    for elt in mc:\n",
    "        mc_membership_dict[elt] = mc_membership_dict[elt] + [t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T16:26:23.411684Z",
     "start_time": "2021-03-09T16:26:01.971824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE OF RUNNING NETMIX\n",
    "\n",
    "# 1. implant anomaly consisting of 3 disjoint cliques in Gdelta\n",
    "S=[max_cliques_delta_sorted[3], max_cliques_delta_sorted[500], max_cliques_delta_sorted[900]]\n",
    "S=list(set().union(*S))\n",
    "\n",
    "# 2. create vertex scores where scores of vertices in S are N(mu,1), other scores are N(0,1)\n",
    "mu=2\n",
    "scores=np.random.normal(size=n)\n",
    "scores[S]+=mu\n",
    "\n",
    "# 3. using scores, fit to GMM and estimate mu, alpha\n",
    "alpha_list=np.array([25/n, 50/n, 75/n, 100/n])\n",
    "mu_est, alpha_est = em(scores, alpha_list)\n",
    "est_size=int(n*alpha_est)\n",
    "resps=compute_responsibilities(scores,mu_est, alpha_est)\n",
    "\n",
    "# 4. Run NetMix (time_limit=time limit for Gurobi in seconds)\n",
    "mc=max_cliques_delta_sorted_sizeabove5\n",
    "mcdict=mc_membership_dict\n",
    "S_netmix,_=netmix_ppr_clique_gurobi(resps, mc, mcdict, k, est_size, time_limit=2*60)\n",
    "\n",
    "# 5. evaluate S_netmix vs S\n",
    "print(fmeasure(S, S_netmix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
